from fastapi import FastAPI, UploadFile, File, Request
from fastapi.middleware.cors import CORSMiddleware
from typing import List
from starlette.responses import JSONResponse
from data_ingestion.ingestion_pipeline import DataIngestion  # you already have this
from agent.workflow import GraphBuilder  # this should be your graph stream handler
from data_models.models import *

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # set specific origins in prod
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/upload")
async def upload_files(files: List[UploadFile] = File(...)):
    try:
        ingestion = DataIngestion()
        ingestion.run_pipeline(files)
        return {"message": "Files successfully processed and stored."}
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})
    

@app.post("/query")
async def query_chatbot(request: QuestionRequest):
    try:
        graph_service = GraphBuilder()
        graph_service.build()
        graph = graph_service.get_graph()
        
        # Assuming request is a pydantic object like: {"question": "your text"}
        messages={"messages": [request.question]}
        
        result = graph.invoke(messages)
        
        # If result is dict with messages:
        if isinstance(result, dict) and "messages" in result:
            final_output = result["messages"][-1].content  # Last AI response
        else:
            final_output = str(result)
        
        return {"answer": final_output}
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})
    

# Assuming this code is at the bottom of the file containing the GraphBuilder class
if __name__ == "__main__":
    
    # Imports needed specifically for the main execution block
    from langchain_core.messages import HumanMessage
    
    try:
        # 1. Instantiate the workflow (calls __init__ and compiles the graph)
        print("Initializing GraphBuilder and compiling LangGraph...")
        # Note: You need to ensure 'GraphBuilder' is defined/imported in this file
        workflow_instance = GraphBuilder() 
        print("✅ Graph Initialization Complete.")
        
        # 2. Define the initial state with a user message
        initial_state = {
            "messages": [HumanMessage(content="What are the benefits of using a leveraged ETF?")]
        }
        
        print("\n--- STARTING CAG WORKFLOW EXECUTION ---")
        
        print("\n--- STARTING EXECUTION STREAM ---")
        print("Node Trace:")

        final_state = {} # Initialize final_state before the loop

        # Use .stream() instead of .invoke()
        for chunk in workflow_instance.graph.stream(initial_state):
            # Print the output of each node as it executes
            print(f"|--- Node Output: {chunk}") 
            
            # Continuously update the final_state with the last chunk
            final_state = chunk 

        # --- Execution is COMPLETE here ---
        
        # 4. Extract and print the final answer
        # Check if final_state was populated before accessing
        if final_state and "messages" in final_state:
            final_message = final_state["messages"][-1]
            
            if hasattr(final_message, 'content'):
                final_answer = final_message.content
            else:
                final_answer = "Error: Final response message was not a standard LangChain message."
                
            print("\n--- FINAL BOT RESPONSE ---")
            print(final_answer)
            print("--------------------------------------\n")
        
        else:
            print("\n❌ EXECUTION FAILED: No final state or messages were generated by the graph.")
            
    except Exception as e:
        print(f"\n❌ CRITICAL ERROR DURING EXECUTION: {e}")
        print("Please check for missing imports, state definition errors, or API key issues.")
        